# -*- coding: utf-8 -*-
"""Code for paper MLP_optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1anTrtH05Q1wTZEzDFRbElQS5Jg5AeLbg
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score, mean_squared_error
from google.colab import files


uploaded = files.upload()
data = pd.read_csv('Data.csv')

input_features = ['Curcumin Solubility', 'Polarity', 'Hildebrand Solubility Parameters',
                  'Dipole Moment', 'Dielectric constants', 'Viscosity', 'delta d', 'delta p', 'delta h']
output_variables = ['EE%', 'DLC%']

X = data[input_features]
y = data[output_variables]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

node_configs = [(50, 25), (100, 50), (200, 100), (150, 75), (100, 100), (300, 150)]
activation_functions = ['relu', 'tanh', 'logistic']
optimizers = ['adam', 'sgd', 'lbfgs']
alphas = [0.00001, 0.0001, 0.001, 0.01]
max_iters = [2000, 4000, 6000]
learning_rates = ['constant', 'invscaling', 'adaptive']

def evaluate(y_true, y_pred):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    return r2, mse

def test_nodes():
    results = {}
    print("\n[Testing hidden layer configurations]")

    for nodes in node_configs:
        config_name = f"Nodes_{nodes}"
        results[config_name] = {}

        for target in output_variables:
            mlp = MLPRegressor(hidden_layer_sizes=nodes, activation='relu', solver='adam',
                               alpha=0.0001, max_iter=6000, random_state=42)

            mlp.fit(X_train_scaled, y_train[target])
            y_pred = mlp.predict(X_test_scaled)
            r2, mse = evaluate(y_test[target], y_pred)

            results[config_name][target] = {'r2': r2, 'mse': mse, 'iterations': mlp.n_iter_}
            print(f"\n{nodes} - {target}")
            print(f"R²: {r2:.4f} | MSE: {mse:.4f} | Iterations: {mlp.n_iter_}")
    return results

def test_activations():
    results = {}
    print("\n[Testing activation functions]")

    for act in activation_functions:
        config_name = f"Activation_{act}"
        results[config_name] = {}

        for target in output_variables:
            mlp = MLPRegressor(hidden_layer_sizes=(100, 50), activation=act, solver='adam',
                               alpha=0.0001, max_iter=6000, random_state=42)

            mlp.fit(X_train_scaled, y_train[target])
            y_pred = mlp.predict(X_test_scaled)
            r2, mse = evaluate(y_test[target], y_pred)

            results[config_name][target] = {'r2': r2, 'mse': mse, 'iterations': mlp.n_iter_}
            print(f"\nActivation: {act} - {target}")
            print(f"R²: {r2:.4f} | MSE: {mse:.4f} | Iterations: {mlp.n_iter_}")
    return results

def test_optimizers():
    results = {}
    print("\n[Testing optimizers]")

    for solver in optimizers:
        config_name = f"Optimizer_{solver}"
        results[config_name] = {}

        for target in output_variables:
            mlp = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver=solver,
                               alpha=0.0001, max_iter=6000, random_state=42)

            mlp.fit(X_train_scaled, y_train[target])
            y_pred = mlp.predict(X_test_scaled)
            r2, mse = evaluate(y_test[target], y_pred)

            results[config_name][target] = {'r2': r2, 'mse': mse, 'iterations': mlp.n_iter_}
            print(f"\nOptimizer: {solver} - {target}")
            print(f"R²: {r2:.4f} | MSE: {mse:.4f} | Iterations: {mlp.n_iter_}")
    return results

def test_alphas():
    results = {}
    print("\n[Testing alpha values]")

    for alpha in alphas:
        config_name = f"Alpha_{alpha}"
        results[config_name] = {}

        for target in output_variables:
            mlp = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam',
                               alpha=alpha, max_iter=6000, random_state=42)

            mlp.fit(X_train_scaled, y_train[target])
            y_pred = mlp.predict(X_test_scaled)
            r2, mse = evaluate(y_test[target], y_pred)

            results[config_name][target] = {'r2': r2, 'mse': mse, 'iterations': mlp.n_iter_}
            print(f"\nAlpha: {alpha} - {target}")
            print(f"R²: {r2:.4f} | MSE: {mse:.4f} | Iterations: {mlp.n_iter_}")
    return results

def test_max_iters():
    results = {}
    print("\n[Testing max_iter values]")

    for max_iter in max_iters:
        config_name = f"MaxIter_{max_iter}"
        results[config_name] = {}

        for target in output_variables:
            mlp = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam',
                               alpha=0.0001, max_iter=max_iter, random_state=42)

            mlp.fit(X_train_scaled, y_train[target])
            y_pred = mlp.predict(X_test_scaled)
            r2, mse = evaluate(y_test[target], y_pred)

            results[config_name][target] = {'r2': r2, 'mse': mse, 'iterations': mlp.n_iter_}
            print(f"\nMax Iter: {max_iter} - {target}")
            print(f"R²: {r2:.4f} | MSE: {mse:.4f} | Iterations: {mlp.n_iter_}")
    return results

def test_learning_rates():
    results = {}
    print("\n[Testing learning rate strategies (SGD only)]")

    for lr in learning_rates:
        config_name = f"LearningRate_{lr}"
        results[config_name] = {}

        for target in output_variables:
            mlp = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='sgd',
                               learning_rate=lr, alpha=0.0001, max_iter=6000, random_state=42)

            mlp.fit(X_train_scaled, y_train[target])
            y_pred = mlp.predict(X_test_scaled)
            r2, mse = evaluate(y_test[target], y_pred)

            results[config_name][target] = {'r2': r2, 'mse': mse, 'iterations': mlp.n_iter_}
            print(f"\nLearning Rate: {lr} - {target}")
            print(f"R²: {r2:.4f} | MSE: {mse:.4f} | Iterations: {mlp.n_iter_}")
    return results

def print_best_configs(results_dict, param_type):
    print(f"\n=== Best Configurations by {param_type} ===")
    for target in output_variables:
        best = max(results_dict, key=lambda x: results_dict[x][target]['r2'])
        print(f"\nTarget: {target}")
        print(f"Best config: {best}")
        print(f"R²: {results_dict[best][target]['r2']:.4f}")
        print(f"MSE: {results_dict[best][target]['mse']:.4f}")
        print(f"Iterations: {results_dict[best][target]['iterations']}")


node_results = test_nodes()
activation_results = test_activations()
optimizer_results = test_optimizers()
alpha_results = test_alphas()
max_iter_results = test_max_iters()
learning_rate_results = test_learning_rates()

print_best_configs(node_results, "Hidden Layers")
print_best_configs(activation_results, "Activation")
print_best_configs(optimizer_results, "Optimizer")
print_best_configs(alpha_results, "Alpha")
print_best_configs(max_iter_results, "Max Iter")
print_best_configs(learning_rate_results, "Learning Rate")