# -*- coding: utf-8 -*-
"""Code_SHAP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oCRWmBphHQjjROlEOBp8o3nIvkwuMAMj
"""

!pip install shap

import random
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt
import shap

from google.colab import files
uploaded = files.upload()

data = pd.read_csv('Data.csv')


input_features = ['Curcumin Solubility', 'Polarity', 'Hildebrand Solubility Parameters',
                  'Dipole Moment', 'Dielectric constants', 'Viscosity', 'delta d', 'delta p', 'delta h']
output_variables = ['EE%', 'DLC%']

X = data[input_features]
y = data[output_variables]

random.seed(1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


def evaluate_model(y_true, y_pred, output_name):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    print(f"MLP - {output_name}:")
    print(f"R-squared: {r2:.4f}")
    print(f"MSE: {mse:.4f}\n")
    return r2, mse


def train_mlp():
    mlp_models = {}
    mlp_params = {
        'hidden_layer_sizes': (100, 50),
        'activation': 'relu',
        'solver': 'adam',
        'alpha': 0.0001,
        'max_iter': 6000,
        'random_state': 42
    }

    for output in output_variables:
        mlp = MLPRegressor(**mlp_params)
        mlp.fit(X_train_scaled, y_train[output])
        mlp_models[output] = mlp

    return mlp_models


print("Training MLP models...")
mlp_models = train_mlp()


results = {}
for output in output_variables:
    print(f"\nEvaluating MLP model for {output}:")
    mlp_pred = mlp_models[output].predict(X_test_scaled)
    results[output] = evaluate_model(y_test[output], mlp_pred, output)

# SHAP analysis
def shap_analysis(model, X, feature_names, output):
    explainer = shap.KernelExplainer(model.predict, X)
    shap_values = explainer.shap_values(X)


    shap_df = pd.DataFrame(shap_values, columns=feature_names)


    shap_df.loc['mean_abs'] = shap_df.abs().mean()


    shap_df = shap_df.sort_values(by='mean_abs', axis=1, ascending=False)


    shap_df = shap_df.drop('mean_abs')

    return shap_df


shap_results = {}
for output in output_variables:
    print(f"\nCalculating SHAP values for {output}")
    shap_results[output] = shap_analysis(mlp_models[output], X_test, input_features, output)


    shap_results[output].to_csv(f'shap_values_{output}.csv')
    print(f"SHAP values for {output} saved to 'shap_values_{output}.csv'")

for output in output_variables:
    print(f"\nSHAP value summary for {output}:")
    print(shap_results[output].describe())




for output in output_variables:
    print(f"\nMLP Model for {output}:")
    print(f"Number of layers: {len(mlp_models[output].coefs_)}")
    print(f"Number of neurons in each layer: {[layer.shape[0] for layer in mlp_models[output].coefs_]}")
    print(f"Activation function: {mlp_models[output].activation}")
    print(f"Solver: {mlp_models[output].solver}")
    print(f"Alpha (L2 penalty): {mlp_models[output].alpha}")
    print(f"Number of iterations: {mlp_models[output].n_iter_}")
